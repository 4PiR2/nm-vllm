window.BENCHMARK_DATA = {
  "lastUpdate": 1710453429921,
  "repoUrl": "https://github.com/neuralmagic/nm-vllm",
  "entries": {
    "Benchmark": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "eccfce583c53190ab168642b2873ff1f921cbdba",
          "message": "move describe gpu to benchmark result",
          "timestamp": "2024-03-14T16:38:04Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/eccfce583c53190ab168642b2873ff1f921cbdba"
        },
        "date": 1710436052468,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "request_throughput (prompts/s)",
            "value": 1.0490477841414547,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "input_throughput (tokens/s)",
            "value": 195.33269740713888,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "output_throughput (tokens/s)",
            "value": 185.4716482362092,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "request_throughput (prompts/s)",
            "value": 6.404859345675221,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "token_throughput (tokens/s)",
            "value": 2904.1553731095155,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "eccfce583c53190ab168642b2873ff1f921cbdba",
          "message": "move describe gpu to benchmark result",
          "timestamp": "2024-03-14T16:38:04Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/eccfce583c53190ab168642b2873ff1f921cbdba"
        },
        "date": 1710436054088,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "median_request_latency (ms)",
            "value": 3377.8040320000855,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p90_request_latency (ms)",
            "value": 4604.980417599995,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p99_request_latency (ms)",
            "value": 4748.848152160017,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "mean_ttft_ms (ms)",
            "value": 163.09148199998162,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "median_ttft_ms (ms)",
            "value": 188.5297860000037,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p90_ttft_ms (ms)",
            "value": 274.76414279999517,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p99_ttft_ms (ms)",
            "value": 309.996213480008,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "mean_tpot_ms (ms)",
            "value": 18.481741989233722,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "median_tpot_ms (ms)",
            "value": 15.528305151078973,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p90_tpot_ms (ms)",
            "value": 24.81720122892424,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p99_tpot_ms (ms)",
            "value": 29.000780437164956,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun",
            "email": "varun@neuralmagic.com"
          },
          "id": "9956cf97467119b9a2202554d13d38877aaf57d2",
          "message": "add github actions",
          "timestamp": "2024-03-14T03:17:29Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/9956cf97467119b9a2202554d13d38877aaf57d2"
        },
        "date": 1710453427811,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "request_throughput (prompts/s)",
            "value": 1.052745278983591,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "input_throughput (tokens/s)",
            "value": 196.02117094674463,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "output_throughput (tokens/s)",
            "value": 186.12536532429888,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "request_throughput (prompts/s)",
            "value": 6.409502400091846,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "token_throughput (tokens/s)",
            "value": 2906.2606732736454,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun",
            "email": "varun@neuralmagic.com"
          },
          "id": "9956cf97467119b9a2202554d13d38877aaf57d2",
          "message": "add github actions",
          "timestamp": "2024-03-14T03:17:29Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/9956cf97467119b9a2202554d13d38877aaf57d2"
        },
        "date": 1710453429654,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "median_request_latency (ms)",
            "value": 3377.3329870000453,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p90_request_latency (ms)",
            "value": 4589.005546999988,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p99_request_latency (ms)",
            "value": 4732.178599999979,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "mean_ttft_ms (ms)",
            "value": 218.7561904000404,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "median_ttft_ms (ms)",
            "value": 321.509041000013,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p90_ttft_ms (ms)",
            "value": 338.34369759999845,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p99_ttft_ms (ms)",
            "value": 348.2297371599816,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "mean_tpot_ms (ms)",
            "value": 15.679003519874161,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "median_tpot_ms (ms)",
            "value": 15.467640302157921,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p90_tpot_ms (ms)",
            "value": 17.179308397103597,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          },
          {
            "name": "p99_tpot_ms (ms)",
            "value": 18.034952006798076,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\"\n}"
          }
        ]
      }
    ]
  }
}