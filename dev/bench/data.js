window.BENCHMARK_DATA = {
  "lastUpdate": 1710575143340,
  "repoUrl": "https://github.com/neuralmagic/nm-vllm",
  "entries": {
    "Benchmark": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "5eb49ae26f01fbf22a6dc978758ad8821c28c36e",
          "message": "fix nm-benchmark workflow input",
          "timestamp": "2024-03-16T01:05:06Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/5eb49ae26f01fbf22a6dc978758ad8821c28c36e"
        },
        "date": 1710575140203,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 11.653877507416118,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2995.0465194059425,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.1277052418386395,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 536.6016814390231,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.638329438933368,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 862.9828270613377,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.56753293863361,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2023.7792820223692,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.445904542671173,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1357.9675905472525,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.515713619849431,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1352.409566770833,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1026.4713786415941,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.18265883224393,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3644.94331988669,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.888741306794439,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3985.9598394643,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.12018726140312,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 563.0431834438531,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839254143665218,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.1136881773502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.39341969653861,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.49192869367029735,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.0364308848064,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 120.41430563661538,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.009912573409565,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 990.3271149225156,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6003048932952669,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 78.0396361283847,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.49194528414167654,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.04081641001076,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 102.03601120143894,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.54672517062774,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3060.9868207787063,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.738349592357323,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3831.808332166256,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.267418555633947,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1352.3022660948416,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 856.6700503149962,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.456998324450882,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3825.4401404433024,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 34.739925108290215,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2258.0951320388644,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839656255905086,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.12554459078007,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.39881993996718,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.43016365153718,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1355.9212746998332,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.08091930480713,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1091.6703370586354,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.514737779075864,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3046.0307959847105,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.555687835595366,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3065.1774044109693,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.19790082436215,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2897.890509438767,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8.363258169648008,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3211.491137144835,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 13.934634940629815,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3581.2011797418622,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.609820797655741,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3390.8380691973953,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.9348595666943056,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3964.527252156632,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.707140266250885,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2929.221094346364,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.439079020408645,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 837.0802726531239,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.313040761055591,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1291.7111398482853,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 980.2808432949979,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 31.098268634334875,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1026.242864933051,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.245472680930572,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 291.9114485209744,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.1620420177998705,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2904.5770977622537,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.767512001078849,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2049.7765601402502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.240047480942533,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 291.2061725225293,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.524264074588295,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1106.300714461414,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.23078948262167,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3709.395004584917,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 37.223422992137266,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 632.7981908663336,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.778702666443245,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3873.170233104326,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.089731553946757,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3621.0610093643163,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.205320666300434,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 513.4904513271074,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.362909342703464,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 567.1782145514503,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.786090860557186,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3286.0253511631972,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9319700711233293,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121.15610924603281,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.8419991157028344,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 499.45988504136847,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.8287594532467213,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3924.4784395778897,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.669259602548794,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 867.0037483313432,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.968575138974044,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4033.6104597578164,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.4919305367022828,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.03691807188144,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121.99877310216614,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 5.188958660304692,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 674.56462583961,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 34.44050605583,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 585.4886029491099,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.49194107456903136,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.03970365157775,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 122.01122531461115,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839226508710164,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 293.38277629455007,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 213.48825704382355,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6025250613030735,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 78.32825796939954,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.4918925109881429,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.0268663546057,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121.98606344165285,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.1737729634538208,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 412.5904852489967,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 23.013879209593902,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2968.790418037613,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.959873283518009,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 124.78352685734117,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18.129895972752557,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2356.886476457832,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 13.530455804029485,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1758.959254523833,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.960133811771467,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 526.3222748001149,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 28.841190108757786,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1874.6773570692562,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839622241339058,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.1245416599626,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 186.36572512504222,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.4533975664306964,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 757.6745924489039,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 563.2706404816907,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.070863718240368,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2089.212283371248,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.8974198908970794,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3887.8133564481154,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.9971330196846437,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4092.1255573338353,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.07336801790486,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1294.23393590759,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 912.0624017317201,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6165777335192218,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 80.15510535749884,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.570303833618972,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1634.1394983704665,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.491926662343676,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 133.02352851989457,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 124.66077526005208,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.106428955570649,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 533.8357642241842,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839512002764076,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.121291238833,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.3924262543311,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.9401759549399091,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3975.4205316718735,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.438208848768921,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1226.96715033996,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.077242558411415,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1292.0373943349935,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 931.9489223846252,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17.602796902237085,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2270.760800388584,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.905488434571522,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2067.713496494298,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.6128036895798887,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3703.1237818193863,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.983184274486542,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1297.8139556832502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.608480252838787,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1380.1922147626042,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 923.7360711596808,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.230373045896392,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1340.5629145141074,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 966.9843113283916,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.513895679552359,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1366.8064383418068,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.4170495285699225,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 746.4493490764872,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 550.0366816521906,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.67606392485187,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 867.8883102307432,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 35.44043871005184,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1169.534477431711,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.2869392109402575,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 817.3020974222335,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.2455984265256816,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 291.9277954483386,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.21195293686301,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1261.4363810767907,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 957.2056164628632,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.417196152358888,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1399.765288721008,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1009.6768069113247,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 23.151863701599762,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2986.5904175063692,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.085856620152306,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2085.5806803099,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.209987747449248,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1232.5707427839588,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 977.5254750557359,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.8595331851867085,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3956.021514816376,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.311367557937389,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3750.7315572218804,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.948885705408092,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3993.26681038118,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9840237970432949,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.14269663753896,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 210.88942002367202,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.44179089699679,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1357.432816609583,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.467712436691684,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2898.3349043332273,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.79540022464135,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2131.701014601688,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.276813349051978,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3733.005248063665,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6421513112821858,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 83.47967046668415,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20.38587360073165,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2629.777694494383,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.726074151213482,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3144.843230141376,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.055122690295775,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1090.8190487797606,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.075738657138646,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3629.8539311121253,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.363788726580626,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 770.9197518504012,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 537.2891775517763,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.3933580900570104,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 739.1328010920064,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 481.92977615799975,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.4061694373479265,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 743.089286771369,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 552.3089244229266,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.383875875963916,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2104.9519319376545,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.80905719338409,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1482.5887175699656,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.012098234460224,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1271.3938095181004,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 912.0488937825922,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.723195629589689,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1414.5482847072728,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1073.4957746858606,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.061790714627772,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3613.8802136593376,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.097019454856469,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 532.612529131341,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.871603929537194,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 575.8172668021323,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.720552549853785,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 873.6718314809921,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.3887238622752656,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 310.5341020957845,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "5eb49ae26f01fbf22a6dc978758ad8821c28c36e",
          "message": "fix nm-benchmark workflow input",
          "timestamp": "2024-03-16T01:05:06Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/5eb49ae26f01fbf22a6dc978758ad8821c28c36e"
        },
        "date": 1710575142649,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 181768.4122569999,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 321864.8054753,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 388032.14075925,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 129173.79456064402,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 127053.87499650032,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 261228.16405309993,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 292523.7905215898,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 251.83016429894383,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 237.16674062645694,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 274.45406319867396,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 933.5797303226809,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1503.1899505002002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 5598.858744399606,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10494.033668820208,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 100.21935252998144,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 60.89209599986134,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 228.36746869988934,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 384.1795628699032,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.29287830638501,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.40225296294245,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.66149766765819,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 27.70921612458737,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2181.248924999636,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8575.459510099972,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15439.345413970897,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 98.16269156662504,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 46.69466350060247,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 221.1274933988534,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 375.63526731937577,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.807040065825392,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.58665640276533,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.352670275886112,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20.684134443166702,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2930.190382499859,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 11103.790156899777,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 19489.0419289192,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 96.76291613326612,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 59.297190000506816,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 199.17227530040685,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 340.70694215009996,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20.038251542136592,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17.415302100955024,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.45922777502407,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 36.20306273191086,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 19399.890889000744,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 84912.07976729986,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 167046.66748070004,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 222.82512209332586,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 177.3890679996839,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 441.47480579886195,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 727.7430322603503,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 182.58466617900217,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 170.66333973942028,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.6856028205374,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 449.77393837926377,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2517.6354905000835,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9167.662407500053,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18027.760980959996,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 108.30397555665589,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 65.78791749996071,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 235.4899520998515,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 448.1937749499433,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17.26224039288815,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.423445585368665,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20.701850689630895,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 38.238598515838724,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 197677.54401599994,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 351276.31550809887,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 419205.34055154095,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 143340.55215815967,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 141076.41774950025,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 288149.4817404007,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 322849.6005550709,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 269.98088094968693,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 249.39335443889405,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 286.22954732461994,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1092.84642753997,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1650.3080439997575,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6526.211662400418,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12088.89194215979,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 94.64789835334159,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 50.529875999927754,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 206.70305920002642,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 370.05008224027705,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.639392693785732,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.516648420345453,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 11.11174868624811,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.037507827352451,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2624.595531500006,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9984.147805999986,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17833.969432629852,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 93.36911931999946,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 47.998789500070416,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 211.28593709996719,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 370.21134221000904,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.515514719567904,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.703007275871524,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.533991629136676,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18.913679305986143,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2414.7998104999715,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8237.037565299988,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17488.598858889807,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 105.27530017666474,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 62.25480900002367,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 240.6208611999319,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 408.1318597100289,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.39698755996804,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.38712233989107,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20.128764467773888,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 36.53442412475175,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1514.988001999427,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 5803.1678897996235,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10771.190068679336,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 86.28804488000847,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 41.345168000134436,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 200.30929010026736,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 347.4356927396726,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8.65017372584424,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8.61617970865286,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.507405147974463,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.611158374375023,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2730.5709949996526,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9823.131688499969,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 19453.596304560608,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 108.22316434997144,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 69.5374040005845,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 222.92386160097533,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 366.6115074998192,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.309414695588224,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 19.527572610208075,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.972362581700224,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 49.946772006383064,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2433.8199335002173,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10118.822185300362,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18699.10280574986,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 125.2894312706667,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 89.68433449990698,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 295.28087580019934,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 486.9409274001827,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18.753007587524454,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.582908248203207,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 26.207495018935628,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 67.05438347103517,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 21965.173906000018,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 106736.75293289933,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 198332.29612929953,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 448.4085668840229,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 244.48172450001948,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1220.8158395999824,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2496.4915651604583,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 189.10699570664173,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 194.46354569997652,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 267.5529068010703,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 343.1966979054768,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2591.1679525002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9490.058997700226,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16869.6587881501,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 91.70379306667807,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 44.72482450023563,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.84278340026503,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 371.2875013200389,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 13.707226882758436,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 13.83937019223286,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.387096518421995,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18.340321325867972,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1740.829450000092,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6936.630111900741,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12291.697877680766,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 110.50589311664832,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 74.33294149905123,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 247.5725857986618,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 402.2695429890877,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.472493021638476,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 11.302734023592363,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.733827827665642,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.126985035829684,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 28544.496605999484,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 117391.96566910014,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 220896.4006744195,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1813.7894412633746,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 382.8007265001361,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 5746.052578600393,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6755.616718571018,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 203.8812138877516,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 216.02112000021913,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 271.6982569649517,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 349.90704549226695,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 175647.80143450026,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 308992.44882209966,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 375024.40238119103,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 123712.80681695468,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121808.03679350083,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 249889.3526492993,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 279392.36249413085,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 293.49012917320624,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 242.7318978279706,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 466.0165780780571,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 895.4378832318181,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20431.23906200003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 91541.23251029996,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 179529.6254769201,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 245.90705426333412,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 199.78548650010453,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 491.75869789974064,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 816.4636607403007,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 160.0211493818793,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 163.3619176494192,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 219.66304240247652,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 293.9893553869122,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3574.648427000284,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15571.383000700373,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 27627.868622600145,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 144.96133089736878,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 112.87505000018427,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 324.29218370107264,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 551.5728594098073,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 28.24526975240189,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 25.09890422312508,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 38.036901196037945,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 88.02903755870676,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 207910.6969170007,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 368874.97395019943,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 437804.9666420698,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 152310.06402923033,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 150608.8493874995,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 305047.92812060076,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 340918.5409017088,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 277.523862393833,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 256.4741867134193,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 293.96466215824944,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1145.2728332915005,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15860.991849499896,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 71916.08288790047,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 143686.1271742795,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 222.01759058532662,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 176.17575099984606,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 463.7023198003592,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 767.2082719098307,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 126.23358275299593,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 129.7326952131216,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 174.78248428621228,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 252.33050228336046,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 203133.88628050007,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 364961.2099099995,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 429736.4474788596,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 148980.11557466164,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 148731.19569949995,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 296636.3177164996,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 331563.0682751599,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 253.64433710398947,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 252.3050231134326,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 282.193543951765,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 629.7202368313897,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2198.4257780004555,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8378.843611099366,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16060.0059833404,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 115.81111014667233,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 73.24041500032763,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 252.96186700052203,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 411.59973673933524,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.366239164776252,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.374415268370091,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18.73276702533665,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 37.49909093134258,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4341.129336499989,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18852.45652120007,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 34446.082809150554,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 141.05492781734515,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 108.07224299969675,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 302.47817979980033,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 524.6777438096975,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.71894381536084,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 31.023464687497913,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 42.79651494125325,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 81.41464155469198,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4220.756829000493,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17926.666714898784,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 31318.460549409025,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 134.46038184535428,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 97.50832750069094,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 284.73720029869594,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 480.8807218290348,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 38.64633797942683,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.303489972354384,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 59.169734299614205,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 120.99912666344896,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4024.5966894999583,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17056.972026400133,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 29864.20584797003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 135.64835794533064,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 101.77333350009121,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 312.49233160010596,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 504.7181713899179,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.22188283499542,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 27.922320543421318,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 38.95886134449843,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 75.40596256917406,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30150.677281500975,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121042.745107601,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 228377.6494602901,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3230.8116631320017,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 439.13615550081886,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10159.446961499458,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12319.445107179708,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 209.31996923972358,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 225.10451073628607,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 280.6977306490389,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 387.0293136591499,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 168973.20880049982,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 300944.6623147999,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 367023.44031591946,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 118922.233527489,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 116235.50223250003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 241778.15288020065,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 271471.5520987402,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 235.38802227209658,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 229.07761126110486,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 264.4135382360815,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 692.6308247140433,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          }
        ]
      }
    ]
  }
}