window.BENCHMARK_DATA = {
  "lastUpdate": 1710575140889,
  "repoUrl": "https://github.com/neuralmagic/nm-vllm",
  "entries": {
    "Benchmark": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "5eb49ae26f01fbf22a6dc978758ad8821c28c36e",
          "message": "fix nm-benchmark workflow input",
          "timestamp": "2024-03-16T01:05:06Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/5eb49ae26f01fbf22a6dc978758ad8821c28c36e"
        },
        "date": 1710575140203,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 11.653877507416118,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2995.0465194059425,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.1277052418386395,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 536.6016814390231,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.638329438933368,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 862.9828270613377,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.56753293863361,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2023.7792820223692,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.445904542671173,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1357.9675905472525,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.515713619849431,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1352.409566770833,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1026.4713786415941,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.18265883224393,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3644.94331988669,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.888741306794439,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3985.9598394643,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.12018726140312,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 563.0431834438531,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839254143665218,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.1136881773502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.39341969653861,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.49192869367029735,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.0364308848064,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 120.41430563661538,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.009912573409565,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 990.3271149225156,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6003048932952669,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 78.0396361283847,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.49194528414167654,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.04081641001076,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 102.03601120143894,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.54672517062774,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3060.9868207787063,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.738349592357323,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3831.808332166256,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.267418555633947,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1352.3022660948416,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 856.6700503149962,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.456998324450882,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3825.4401404433024,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 34.739925108290215,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2258.0951320388644,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839656255905086,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.12554459078007,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.39881993996718,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.43016365153718,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1355.9212746998332,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.08091930480713,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1091.6703370586354,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.514737779075864,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3046.0307959847105,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.555687835595366,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3065.1774044109693,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.19790082436215,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2897.890509438767,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 8.363258169648008,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3211.491137144835,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 13.934634940629815,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3581.2011797418622,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.609820797655741,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3390.8380691973953,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.9348595666943056,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3964.527252156632,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.707140266250885,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2929.221094346364,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.439079020408645,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 837.0802726531239,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.313040761055591,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1291.7111398482853,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 980.2808432949979,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 31.098268634334875,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1026.242864933051,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.245472680930572,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 291.9114485209744,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.1620420177998705,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2904.5770977622537,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.767512001078849,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2049.7765601402502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.240047480942533,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 291.2061725225293,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.524264074588295,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1106.300714461414,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.23078948262167,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3709.395004584917,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 37.223422992137266,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 632.7981908663336,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.778702666443245,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3873.170233104326,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.089731553946757,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3621.0610093643163,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.205320666300434,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 513.4904513271074,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.362909342703464,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 567.1782145514503,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.786090860557186,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3286.0253511631972,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9319700711233293,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121.15610924603281,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.8419991157028344,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 499.45988504136847,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.8287594532467213,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3924.4784395778897,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.669259602548794,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 867.0037483313432,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.968575138974044,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4033.6104597578164,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.4919305367022828,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.03691807188144,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121.99877310216614,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 5.188958660304692,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 674.56462583961,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 34.44050605583,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 585.4886029491099,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.49194107456903136,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.03970365157775,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 122.01122531461115,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839226508710164,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 293.38277629455007,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 213.48825704382355,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6025250613030735,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 78.32825796939954,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.4918925109881429,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 130.0268663546057,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 121.98606344165285,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.1737729634538208,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 412.5904852489967,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 23.013879209593902,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2968.790418037613,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.959873283518009,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 124.78352685734117,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 18.129895972752557,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2356.886476457832,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 13.530455804029485,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1758.959254523833,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 30.960133811771467,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 526.3222748001149,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 28.841190108757786,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1874.6773570692562,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839622241339058,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.1245416599626,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 186.36572512504222,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.4533975664306964,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 757.6745924489039,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 563.2706404816907,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 16.070863718240368,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2089.212283371248,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.8974198908970794,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3887.8133564481154,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.9971330196846437,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4092.1255573338353,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.07336801790486,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1294.23393590759,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 912.0624017317201,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6165777335192218,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 80.15510535749884,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 12.570303833618972,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1634.1394983704665,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.491926662343676,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 133.02352851989457,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 124.66077526005208,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.106428955570649,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 533.8357642241842,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9839512002764076,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.121291238833,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 212.3924262543311,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.9401759549399091,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3975.4205316718735,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.438208848768921,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1226.96715033996,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.077242558411415,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1292.0373943349935,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 931.9489223846252,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 17.602796902237085,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2270.760800388584,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 15.905488434571522,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2067.713496494298,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.6128036895798887,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3703.1237818193863,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 9.983184274486542,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1297.8139556832502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.608480252838787,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1380.1922147626042,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 923.7360711596808,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.230373045896392,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1340.5629145141074,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 966.9843113283916,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.513895679552359,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1366.8064383418068,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.4170495285699225,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 746.4493490764872,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 550.0366816521906,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.67606392485187,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 867.8883102307432,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 35.44043871005184,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1169.534477431711,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.2869392109402575,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 817.3020974222335,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.2455984265256816,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 291.9277954483386,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.21195293686301,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1261.4363810767907,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 957.2056164628632,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.417196152358888,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1399.765288721008,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1009.6768069113247,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 23.151863701599762,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2986.5904175063692,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.085856620152306,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2085.5806803099,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.209987747449248,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1232.5707427839588,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 977.5254750557359,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3.8595331851867085,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3956.021514816376,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.311367557937389,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3750.7315572218804,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1.948885705408092,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3993.26681038118,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.9840237970432949,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 290.14269663753896,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 210.88942002367202,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 10.44179089699679,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1357.432816609583,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.467712436691684,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2898.3349043332273,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.79540022464135,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2131.701014601688,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.276813349051978,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3733.005248063665,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 0.6421513112821858,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 83.47967046668415,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 20.38587360073165,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2629.777694494383,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.726074151213482,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3144.843230141376,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.055122690295775,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1090.8190487797606,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 7.075738657138646,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3629.8539311121253,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.363788726580626,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 770.9197518504012,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 537.2891775517763,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.3933580900570104,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 739.1328010920064,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 481.92977615799975,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.4061694373479265,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 743.089286771369,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 552.3089244229266,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 32.383875875963916,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2104.9519319376545,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 22.80905719338409,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1482.5887175699656,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.012098234460224,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1271.3938095181004,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 912.0488937825922,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.723195629589689,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1414.5482847072728,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 1073.4957746858606,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 14.061790714627772,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 3613.8802136593376,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 4.097019454856469,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 532.612529131341,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 33.871603929537194,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 575.8172668021323,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 6.720552549853785,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 873.6718314809921,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 2.3887238622752656,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"benchmarking_context\": {\"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\", \"torch_cuda_version\": \"12.1\", \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", \"cuda_device_names\": [\"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\", \"NVIDIA A10G\"]}}",
            "value": 310.5341020957845,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          }
        ]
      }
    ]
  }
}