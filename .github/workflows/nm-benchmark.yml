name: benchmark 
on:
  # makes workflow reusable
  workflow_call:
    inputs:
      label:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      benchmark_config_list_file:
        description: "path to a file containing a list of benchmark-configs to run benchmarks with. For reference look at .github/data/nm_benchmark_configs_list.txt"
        type: string
        required: true
      timeout:
        description: "maximum time runner will be up"
        type: string
        required: true
      gitref:
        description: "git commit hash or branch name"
        type: string
        required: true
      Gi_per_thread:
        description: 'requested GiB to reserve per thread'
        type: string
        required: true
      python:
        description: "python version, e.g. 3.10.12"
        type: string
        required: true
      push_benchmark_results_to_gh_pages:
        description: "When set to true, the workflow pushes all benchmarking results to gh-pages UI"
        type: string
        required: true

  # makes workflow manually callable
  workflow_dispatch:
    inputs:
      label:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      benchmark_config_list_file:
        description: "path to a file containing a list of benchmark-configs to run benchmarks with. For reference look at .github/data/nm_benchmark_configs_list.txt"
        type: string
        required: true
      timeout:
        description: "maximum time runner will be up"
        type: string
        required: true
      gitref:
        description: "git commit hash or branch name"
        type: string
        required: true
      Gi_per_thread:
        description: 'requested GiB to reserve per thread'
        type: string
        required: true
      python:
        description: "python version, e.g. 3.10.12"
        type: string
        required: true
      push_benchmark_results_to_gh_pages:
        description: "When set to true, the workflow pushes all benchmarking results to gh-pages UI"
        type: choice
        options:
          - 'true'
          - 'false'
        default: 'false'

jobs:
  BENCHMARK:

    runs-on: ${{ inputs.label }}
    timeout-minutes: ${{ fromJSON(inputs.timeout) }}
    outputs:
      gh_action_benchmark_input_artifact_name: ${{ steps.set_gh_action_benchmark_input_artifact_name.outputs.gh_action_benchmark_input_artifact_name}}

    steps:
      - name: checkout repository code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ inputs.gitref }}
          submodules: recursive

      - name: setenv
        id: setenv
        uses: ./.github/actions/nm-set-env/
        with:
          hf_token: ${{ secrets.NM_HF_TOKEN }}
          Gi_per_thread: ${{ inputs.Gi_per_thread }}

      - name: set python
        id: set_python
        uses: ./.github/actions/nm-set-python/
        with:
          python: ${{ inputs.python }}
          venv: TEST

      - name: hf cache
        id: hf_cache
        uses: ./.github/actions/nm-hf-cache/
        with:
          fs_cache: ${{ secrets.HF_FS_CACHE }}

      #######################################################
      - name: announce pause
        run: |
          M=${{ inputs.timeout }}
          R=$((M - 15))
          S=$((R * 60))
          echo "pausing for, ${S} minutes"

      - name: pause workflow
        run: |
          M=${{ inputs.timeout }}
          R=$((M - 15))
          S=$((R * 60))
          sleep $S