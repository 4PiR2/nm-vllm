window.BENCHMARK_DATA = {
  "lastUpdate": 1710831181339,
  "repoUrl": "https://github.com/neuralmagic/nm-vllm",
  "entries": {
    "bigger_is_better": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "username": "varun-sundar-rabindranath",
            "email": "varunsundar08@gmail.com"
          },
          "committer": {
            "name": "GitHub",
            "username": "web-flow",
            "email": "noreply@github.com"
          },
          "id": "f90ec1cbf74fa01503a9dccc7afbaa8715f576a4",
          "message": "Fix nightly  - 03/18/2024 (#136)\n\nSUMMARY:\r\nMiscellaneous changes to fix nightly:\r\n * Benchmarks:\r\n   - Add a benchmark name so the alert-triggering is correct\r\n- Don't skip `github-action-benchmark` failure based on previous failure\r\n- Shorten metric names so `github-action-benchmark` doesn't hit the\r\ngithub comment size threshold\r\n * Nightly-SOLO:\r\n   - Fix code-coverage artifact name\r\n * Misc:\r\n- Add extra information to `github-action-benchmark` JSON that is useful\r\nin the UI\r\n\r\nTEST PLAN:\r\nManual testing\r\n\r\n---------\r\n\r\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>",
          "timestamp": "2024-03-18T23:58:12Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/f90ec1cbf74fa01503a9dccc7afbaa8715f576a4"
        },
        "date": 1710831177983,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9839186075537723,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290.1116811659216,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 212.39195035524912,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.217766357906463,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1336.567981156979,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 964.1167169999295,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9839527814068058,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290.1217574404014,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 212.49772251521847,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15.428244836595477,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:38:59 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2005.671828757412,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:38:59 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.495041745355275,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:32:06 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1624.3554268961857,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:32:06 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9.431745225659517,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:30:53 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1226.1268793357372,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:30:53 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9839796155979668,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 293.39976184694973,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 213.50389699244684,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 29.238407608808572,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:06:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1900.4964945725571,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:06:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.4526082433675502,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 757.4308284383893,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 563.0632602236732,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.7925435014755324,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:28:48 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3887.357089012421,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:28:48 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.083763979594355,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1294.1039674936553,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 933.434100343841,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.399224266664285,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 740.944432859709,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 550.7115402387423,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 22.87289352391561,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:15:22 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1486.7380790545146,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:15:22 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.637743505366036,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:08:37 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3247.900080879071,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:08:37 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 7.176484972496577,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:00:46 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3681.536790890744,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:00:46 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.6386233475456284,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:33:14 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 83.0210351809317,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:33:14 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 7.078806684221659,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:09:44 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3631.427829005711,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:09:44 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.528059485934523,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1356.1070260893682,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1029.2098089149733,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 33.83147816882602,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:32:21 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1116.4387795712585,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:32:21 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 13.93523069179165,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:59:38 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3581.3542877904542,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:59:38 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16.029675489773204,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:18:11 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2083.857813670517,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:18:11 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 33.94939028407503,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:23:15 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1120.329879374476,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:23:15 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9596519703127205,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:19:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 124.75475614065367,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:19:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.192404343785308,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:51:57 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2895.3205749802582,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:51:57 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32.89032910401961,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:43:12 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2137.8713917612745,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:43:12 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 18.157041165975567,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:25:04 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2360.4153515768235,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:25:04 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.491950897968658,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 133.03008215603137,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 124.66035754525794,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.0992486092617515,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:50:12 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 532.9023192040277,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:50:12 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 7.280564655068441,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:37:10 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3734.92966805011,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:37:10 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.4919228246379114,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 130.03487946478552,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 120.68178682567002,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15.690996153602407,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:46:29 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2039.829499968313,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:46:29 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.546957496733268,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:03:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3061.095447172607,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:03:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 29.343422148957828,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:05:16 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 968.3329309156082,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:05:16 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 36.11088250974342,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:22:09 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 613.8850026656381,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:22:09 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.102493680612561,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:35:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3624.340875917428,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:35:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1.959609462532708,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:39:35 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4015.2397887295188,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:39:35 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.044949283459347,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:22:43 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1305.8434068497152,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:22:43 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.556047214372847,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:11:09 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3065.3454355521685,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - 2:4 Sparse (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:11:09 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.016810832190122,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1272.8871846127277,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 915.7257547838227,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 13.531756847523814,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:23:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1759.128390178096,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:23:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15.888657444497307,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:53:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2065.5254677846497,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 64,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 64,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:53:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17.51346855712288,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:16:29 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2259.2374438688516,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:16:29 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.074032541201369,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:14:40 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 529.624230356178,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:14:40 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.388930036120385,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:17:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1350.5609046956502,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:17:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.2170164864519246,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:13:30 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 288.21214323875023,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:13:30 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.489840268440735,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:52:41 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1363.6792348972956,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:52:41 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1.9463877764005562,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:11:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3988.1485538447396,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:11:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.520403642801777,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:36:40 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 847.6524735642311,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:36:40 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 34.54528439387231,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:40:47 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 587.2698346958292,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:40:47 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.267380272352407,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1352.2901345057542,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 856.9297875709052,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1.9081044822228623,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:21:02 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3909.706084074645,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:21:02 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 20.06624992822618,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:07:30 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2588.546240741178,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:07:30 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.324349341316379,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1295.097942781062,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 982.6031619801753,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.622269139435227,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:51:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 860.8949881265796,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:51:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.8232643960110053,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:20:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 497.0243714814307,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:20:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 22.951713884791396,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:34:46 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2960.77109113809,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:34:46 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 23.20290431681759,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:25:27 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2993.1746568694693,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:25:27 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.419207640752157,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:45:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1354.4969932977806,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:45:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.0248530585969813,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:29:56 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4148.923917065215,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:29:56 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.6142763014135991,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:40:14 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 79.85591918376788,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:40:14 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 22.961842560923493,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:58:30 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2962.077690359131,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:58:30 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 29.987869607534673,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:04:09 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 509.79378332808943,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:04:09 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32.44446213716631,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:57:23 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2108.8900389158102,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:57:23 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.6883001323588065,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:47:44 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3127.1816098856834,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:47:44 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.406570432068196,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:34:23 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 312.85415616886553,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:34:23 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.8729066489447503,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:01:54 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3969.7293151683693,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:01:54 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.115921591561094,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:42:42 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 535.0698069029421,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:42:42 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9839302024708598,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290.1150999658746,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 212.37477466865195,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.39572286413871,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 739.8631063890773,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 478.6047366090227,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.201821841750221,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:27:22 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 416.2368394275287,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:27:22 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.519821599310789,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:43:46 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3048.4077869737525,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:43:46 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1.956063305083001,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:49:16 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4007.973712115069,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:49:16 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.8786886295230123,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:48:03 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3975.6558452610875,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:48:03 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9839856817563846,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290.1314582181425,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 185.03850745428812,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.49193518625872273,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 130.03814713563077,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 122.00976489588841,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.461344340638886,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:37:49 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1359.9747642830553,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 32\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 32,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:37:49 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.6052346090098714,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:12:21 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 78.68049917128327,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:12:21 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.362329994915739,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:35:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 567.1028993390461,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:35:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.3686518377204515,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 772.5057939492475,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 538.3756134991569,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.237975060114565,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:41:28 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290.93675781489344,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:41:28 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.49188736088344975,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 130.0255049759311,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 121.99134474816809,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 34.35765528912806,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:55:08 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 584.080139915177,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:55:08 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 33.83223245430643,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:31:08 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 575.1479517232093,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:31:08 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.6675738764901795,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:19:53 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3759.263223402434,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:19:53 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 31.031777748853624,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:14:14 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1024.0486657121696,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:14:14 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32.345353148125646,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:33:34 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2102.447954628167,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:33:34 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 7.332134358165357,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:46:51 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3761.384925738828,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:46:51 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 34.67485612674781,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:24:21 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2253.8656482386077,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 64,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 64,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:24:21 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.7695894537815855,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:10:51 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3863.829190126125,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:10:51 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 33.93942355696572,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:56:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1120.0009773798688,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:56:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.701263098899502,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:18:45 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3437.7479697354443,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:18:45 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.709121215429429,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1410.3331431018876,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1070.3534278327447,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.49191223158841124,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 130.03207929808065,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 121.98439518929422,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.428173522339083,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:29:42 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 835.6625579040808,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:29:42 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 11.615998417116817,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:17:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2985.311593199022,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:17:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 5.091128955479468,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:28:32 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 661.8467642123309,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:28:32 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1.924124501557944,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:03:02 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3942.531103692227,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2048,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2048,\n    \"output_len\": 1,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:03:02 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.6318419444524395,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:43:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 862.1394527788171,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:43:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 8.315750611968816,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:52:32 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3193.2482349960255,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - synthetic\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:52:32 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3.892330418245492,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:38:22 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3989.638678701629,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 1024,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 1024,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:38:22 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 31.428755959385505,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:13:06 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 534.2888513095536,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 16,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 16,\n    \"output_len\": 1,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:13:06 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.4919457120660308,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 130.0409295275346,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 103.10854161095962,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.609555875954635,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1380.5143527710284,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 924.5524507078491,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.224088551976798,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1265.0708724020137,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 959.9522840270738,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.172054502448429,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:56:45 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2909.2966426011058,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine throughput - Dense (with dataset)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1000\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1000,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 04:56:45 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9433888430616232,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:26:13 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 122.64054959801102,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:26:13 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.6117813849146969,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:47:43 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 79.5315800389106,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:47:43 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.651139711255816,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:15:49 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 864.6481624632561,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 16\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 16,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:15:49 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.2157844550337575,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1234.2678622535984,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 978.8812610135933,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 7.52226883066602,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:27:41 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3858.9239101316684,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 512,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 512,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:27:41 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.076611754150917,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1295.26457038754,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 912.7941378719316,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.238474883009656,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:48:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 291.00173479125533,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 4,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 4,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:48:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 23.398470072064267,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:44:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3018.4026392962905,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 128,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 128,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:44:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.416197396160986,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 746.1861878650768,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 551.5341040338195,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4.419228710985807,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1400.4093862242921,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1009.7554604780948,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.008443757629983,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:26:34 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3600.1700457109055,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - Dense (synthetic)\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:26:34 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.04770956163205,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:45:36 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3610.261357339437,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 256,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 256,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:45:36 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6.631816273208401,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:21:34 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 862.1361155170921,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine decode throughput - Dense (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 2,\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 8\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": null,\n    \"input_len\": 2,\n    \"output_len\": 128,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 8,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 05:21:34 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2.401657816872747,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 741.6959780587542,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 551.2957485475054,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 0.9839558883229262,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"input_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290.1226735249759,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"output_throughput\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 209.7793953904479,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"request_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 33.00021704146777,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:41:59 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          },
          {
            "name": "{\"name\": \"token_throughput\", \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1089.0071623684364,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Engine prefill throughput - 2:4 Sparse (synthetic)\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"input-len\\\": 32,\\n  \\\"output-len\\\": 1,\\n  \\\"num-prompts\\\": 1,\\n  \\\"sparsity\\\": \\\"semi_structured_sparse_w16a16\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": \"semi_structured_sparse_w16a16\",\n    \"dataset\": null,\n    \"input_len\": 32,\n    \"output_len\": 1,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 1,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  },\n  \"date\": \"2024-03-19 06:41:59 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"synthetic\"\n}"
          }
        ]
      }
    ],
    "smaller_is_better": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "username": "varun-sundar-rabindranath",
            "email": "varunsundar08@gmail.com"
          },
          "committer": {
            "name": "GitHub",
            "username": "web-flow",
            "email": "noreply@github.com"
          },
          "id": "f90ec1cbf74fa01503a9dccc7afbaa8715f576a4",
          "message": "Fix nightly  - 03/18/2024 (#136)\n\nSUMMARY:\r\nMiscellaneous changes to fix nightly:\r\n * Benchmarks:\r\n   - Add a benchmark name so the alert-triggering is correct\r\n- Don't skip `github-action-benchmark` failure based on previous failure\r\n- Shorten metric names so `github-action-benchmark` doesn't hit the\r\ngithub comment size threshold\r\n * Nightly-SOLO:\r\n   - Fix code-coverage artifact name\r\n * Misc:\r\n- Add extra information to `github-action-benchmark` JSON that is useful\r\nin the UI\r\n\r\nTEST PLAN:\r\nManual testing\r\n\r\n---------\r\n\r\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>",
          "timestamp": "2024-03-18T23:58:12Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/f90ec1cbf74fa01503a9dccc7afbaa8715f576a4"
        },
        "date": 1710831180554,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1746.9751005000944,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6939.2963098000155,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12528.239303990034,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 109.56407162329317,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 74.13111950063467,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 242.02516009881958,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 400.77121956028304,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.501530566054361,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 11.384699604761018,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16.530619358048135,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 30.16093324647077,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:42:54 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 20934.058494500165,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 93266.15278120003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 182842.4658242603,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 240.86501046000058,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 197.9340025000056,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 482.27143820013225,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 791.4041989799216,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 163.60609308912038,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 168.14237534035286,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 223.16172382894703,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 300.2583068258807,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:58:53 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2515.1734594999198,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9478.638828799902,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17916.419126929984,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 107.77685659332444,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 67.3158519999788,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 240.7319551999081,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 392.60241530994057,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17.224175412077546,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16.43353957424866,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 20.76168343563268,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 37.352690176005694,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:45:16 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2442.5815329998386,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 8198.475915000065,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17378.780377050058,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 105.25501574667487,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 61.39706599992678,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 241.28369790018957,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 415.3957259898561,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16.442851538892004,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15.384314816775783,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 20.15785484320284,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 36.71494960376532,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:24:47 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2424.9027700002443,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9938.114032800242,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 18299.568632519822,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 125.07054965599428,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 90.32450999939101,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 295.3219822000394,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 490.4199329902985,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 18.512969410554547,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16.30572038566089,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 26.2411998602055,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 66.69625409902343,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:10:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 28070.0631109994,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 116097.64545030013,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 217951.22164209033,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1498.634523331976,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 369.55214499994327,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4845.089579200432,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6041.424469978483,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 202.69628210870007,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 212.97724416554433,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 269.47397744254056,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 341.11945666913056,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:56:43 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4415.410706499642,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 18330.55999999906,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32243.643520310336,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 143.19639524001832,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 110.10478499974852,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 325.7487118991777,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 523.2086015903587,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32.917187731378924,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 30.429311270134868,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 44.78372493528411,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 82.63018373854437,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:49:20 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 180878.85490600002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 320168.65326539986,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 386466.44697289,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 128320.23090140166,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 126059.8570694999,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 259218.9530594997,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 290989.64684989024,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 249.95304661249958,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 237.16568343332938,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 272.3127474702717,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 930.097252701919,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:11:15 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2596.2625805002517,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9444.9988571002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16838.737749339904,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 92.49171766667132,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 45.82807049996518,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 212.61864999983118,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 371.88172943011955,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 13.647875619102567,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 13.792765900180761,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15.160762676996123,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 18.373011098798628,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:18:31 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2305.5923229994733,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 8814.60860130046,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15592.581829540233,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 97.80787058006051,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 48.04641200007609,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 221.27252219997897,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 372.68944497043753,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.903800277814748,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.81700611515294,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.392775282635224,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 20.675974301696453,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:19:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 29888.196538001466,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 120327.39291680002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 227289.1390777593,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3017.135163384695,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 391.10383150000416,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9661.945217599898,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 11902.70134517963,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 207.56308823842363,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 222.28620566671208,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 281.38497276029835,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 383.1494003771397,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:41:18 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19574.806878000345,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 85732.80072559921,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 167679.6337033693,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 227.9769541446573,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 176.0569250000117,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 452.9644380005266,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 783.1067651508598,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 184.70777342346693,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 170.09850594720615,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 303.52443359473637,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 478.44898255771466,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:26:24 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 197422.18887650053,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 350171.86226050014,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 417717.90864474856,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 142815.91169435935,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 140428.03818749962,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 287266.93956649903,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 321617.26257369004,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 269.48904265777594,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 249.21165739300312,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 286.0297774886676,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1092.9793188093806,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 03:09:38 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1483.0497505004132,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 5715.913968899807,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10258.438498140587,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 100.66786730334873,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 62.73745450016577,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 227.7265196001281,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 384.13970376993285,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.30635523624642,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9.41687873523421,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.427553161931908,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 27.822816451241543,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:04:40 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4388.0485945001055,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 18325.359270199988,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32017.54494271006,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 134.5176232626351,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 101.10717449970252,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 287.0080530996347,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 464.1880664990276,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 39.725315734985735,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 34.42999926185959,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 61.514389520161785,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 109.86417063065396,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:18:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2730.8756865004398,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9896.515330199694,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19787.694182459545,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 109.32423226334019,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 67.71786649915157,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 223.47767740011483,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 372.4530161199978,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 22.52575012225865,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19.543890537519054,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 34.334800225150396,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 48.12743582942016,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:11:57 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2633.5494479999966,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10068.872559999976,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17930.13630854987,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 92.6592437199982,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 44.195319499976904,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 210.72332220009002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 363.0999396899866,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.686820727573314,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.84155859682859,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16.71862453072956,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 20.001186579111977,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:39:00 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4291.840456499813,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19002.759636799557,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 34698.218615699974,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 140.19411618401622,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 106.80386250032825,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 302.5425949995679,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 527.5778484899635,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 32.39315969992204,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 30.691877819661574,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 42.654225678777024,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 81.68812578532709,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:31:16 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1517.5329689996033,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 5895.412842400127,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10817.614545210163,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 86.66953626003306,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 41.25414800000726,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 203.01873539947334,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 353.13250212996513,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 8.5849346048819,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 8.530590394228419,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9.536461245750624,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12.406804291732,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:58:23 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 170753.71439399986,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 303240.7489587005,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 369194.4710858905,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 120166.53432220765,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 117619.04693650012,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 243598.67738249985,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 274013.1599212808,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 237.7713370786222,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 229.90476412457053,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 263.53775998112894,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 727.7725867337284,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:29:50 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1657.9717610002263,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 6569.333387600363,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 12025.861700070052,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 94.51856139336694,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 51.128068000252824,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 211.64880940023062,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 367.9637827699477,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9.573305263580215,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 9.476364134911423,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10.906913883190983,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.066048566718507,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"tokenizer\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'tokenizer': 'TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:36:37 UTC\",\n  \"model\": \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2939.664587000152,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 10917.515488898787,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19595.878908929477,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 95.41389278661275,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 58.666254499257775,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 195.38732589971914,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 332.05518827950044,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19.94333745493508,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17.391109768012377,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 30.609102290474677,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 35.765176297002824,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"150,0.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      150,\n      \"0.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:05:13 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 175449.38171249942,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 309080.7374139014,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 374988.69215483905,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 123645.09518933,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 121804.67549900003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 249681.69712460038,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 280070.9949952192,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 292.9470365275818,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 243.02426922429336,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 466.6737479169784,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 873.3319517384521,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - 2:4 Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\\nmax-model-len - 4096\\nsparsity - semi_structured_sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'semi_structured_sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 04:39:02 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 206087.0038635003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 366551.0936865003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 435705.4449421206,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 151123.96824455468,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 149038.3484845006,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 303024.81096159935,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 338680.9228697209,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 276.70666837276286,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 255.73205134062204,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 292.64992902503434,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1136.798770819434,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:54:58 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 202943.100555,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 364855.08439580014,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 429512.8041773001,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 148768.94288126435,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 148556.2602619998,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 297024.6507920003,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 331554.0625582898,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 253.9808382365661,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 252.70005300059682,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 282.3308798349177,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 630.5546354305361,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"3000,10\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      3000,\n      \"10.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:51:50 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 21988.829340499706,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 106874.18857830044,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 198254.06236031934,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 422.0611001373327,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 237.13120000002164,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 1050.0398217006482,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2198.1255356502425,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 189.76715073133494,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 193.2727121991587,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 266.1973132338244,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 339.7052857891369,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - NousResearch/Llama-2-7b-chat-hf\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"tokenizer\": \"NousResearch/Llama-2-7b-chat-hf\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'NousResearch/Llama-2-7b-chat-hf', 'tokenizer': 'NousResearch/Llama-2-7b-chat-hf', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 01:38:39 UTC\",\n  \"model\": \"NousResearch/Llama-2-7b-chat-hf\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 3548.2797129989194,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15507.835587999582,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 26978.083970829808,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 145.9122281893142,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 115.04302400044253,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 325.1039633005348,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 562.4202051200698,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 27.79666627072162,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 24.829931683582757,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 37.67021490554422,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 79.05739957497792,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:33:19 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16036.010215999795,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 71744.16097620002,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 142352.26692373006,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 220.36655344799874,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 177.71790999995574,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 452.68764550010053,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 751.3712020203001,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 125.73354478118671,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 130.2441714248359,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 170.93289698632017,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 251.2511104573752,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"1500,5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      1500,\n      \"5.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-marlin', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 02:17:53 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 4070.5437429999165,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 17407.078864799973,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 30302.0763944201,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 136.37600092666406,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 102.05976500003544,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 304.77485379994965,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 520.1094485100973,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 30.73214922713832,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 28.051313986612502,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 40.34572667885685,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 79.13489421299845,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Dense\\nmodel - teknium/OpenHermes-2.5-Mistral-7B\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"750,2.5\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"tokenizer\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      750,\n      \"2.5\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'teknium/OpenHermes-2.5-Mistral-7B', 'tokenizer': 'teknium/OpenHermes-2.5-Mistral-7B', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  },\n  \"date\": \"2024-03-19 00:51:41 UTC\",\n  \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 2193.7446375004583,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 8290.757974499322,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_request_latency\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 16092.09616931938,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 114.84066320999773,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 73.85347350009397,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 249.87756979971903,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_ttft_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 407.9551142608757,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"mean_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 15.445943616707085,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"median_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 14.361914611512093,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p90_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 19.2032284190881,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          },
          {
            "name": "{\"name\": \"p99_tpot_ms\", \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\", \"gpu_description\": \"NVIDIA A10G x 4\", \"vllm_version\": \"0.1.0\", \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\", \"torch_version\": \"2.1.2+cu121\"}",
            "value": 36.801621956764755,
            "unit": "ms",
            "extra": "{\n  \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"VLLM Serving - Sparse\\nmodel - neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\\nmax-model-len - 4096\\nsparsity - sparse_w16a16\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"300,1\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"tokenizer\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      300,\n      \"1.0\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'tokenizer': 'neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': '', 'sparsity': 'sparse_w16a16'}\"\n  },\n  \"date\": \"2024-03-19 03:26:26 UTC\",\n  \"model\": \"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50\",\n  \"dataset\": \"sharegpt\"\n}"
          }
        ]
      }
    ]
  }
}