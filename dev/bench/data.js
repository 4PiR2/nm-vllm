window.BENCHMARK_DATA = {
  "lastUpdate": 1710532183588,
  "repoUrl": "https://github.com/neuralmagic/nm-vllm",
  "entries": {
    "Benchmark": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "c5c4aa3147f520acf3811b65ee13a4fd33ffd378",
          "message": "mark push_benchmark_results_to_gh_pages -> false",
          "timestamp": "2024-03-15T15:08:30Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/c5c4aa3147f520acf3811b65ee13a4fd33ffd378"
        },
        "date": 1710517993542,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 1.0528414188865238,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"input_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 196.03907219667076,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"output_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 185.93179457536013,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 6.412056489892468,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"token_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 2907.418774211942,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "c5c4aa3147f520acf3811b65ee13a4fd33ffd378",
          "message": "mark push_benchmark_results_to_gh_pages -> false",
          "timestamp": "2024-03-15T15:08:30Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/c5c4aa3147f520acf3811b65ee13a4fd33ffd378"
        },
        "date": 1710517995188,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"median_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 3383.1900989999895,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4589.934134599957,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4731.8899991599665,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 140.64717260000634,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 139.91662900002666,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 252.32339400004093,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 302.8906050000569,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 15.208553373803998,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 15.560029541516103,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 17.172547212377456,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 18.075282873137372,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "c5c4aa3147f520acf3811b65ee13a4fd33ffd378",
          "message": "mark push_benchmark_results_to_gh_pages -> false",
          "timestamp": "2024-03-15T15:08:30Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/c5c4aa3147f520acf3811b65ee13a4fd33ffd378"
        },
        "date": 1710520268091,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 6.432256610536486,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"token_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 2916.578114915559,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 1.055510395332166,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"input_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 196.53603561084932,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"output_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 186.61423789472695,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "c5c4aa3147f520acf3811b65ee13a4fd33ffd378",
          "message": "mark push_benchmark_results_to_gh_pages -> false",
          "timestamp": "2024-03-15T15:08:30Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/c5c4aa3147f520acf3811b65ee13a4fd33ffd378"
        },
        "date": 1710520269473,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"median_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 3364.5725630000243,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4576.581941399945,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4719.782354039912,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 154.81378039999072,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 47.05497600002673,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 317.06708159999835,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 317.17373916000724,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 14.980379679425152,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 15.225386801947987,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 16.95783004762767,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 17.870837545269037,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "c5c4aa3147f520acf3811b65ee13a4fd33ffd378",
          "message": "mark push_benchmark_results_to_gh_pages -> false",
          "timestamp": "2024-03-15T15:08:30Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/c5c4aa3147f520acf3811b65ee13a4fd33ffd378"
        },
        "date": 1710520796473,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 1.0654392681286293,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"input_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 198.3847917255508,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"output_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 188.36966260514168,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 6.410313064274521,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"token_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 2906.6282527339963,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "c5c4aa3147f520acf3811b65ee13a4fd33ffd378",
          "message": "mark push_benchmark_results_to_gh_pages -> false",
          "timestamp": "2024-03-15T15:08:30Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/c5c4aa3147f520acf3811b65ee13a4fd33ffd378"
        },
        "date": 1710520798972,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"median_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 3332.390385999929,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4535.250369999972,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4675.8589647999315,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 147.15998440001385,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 146.93955600000663,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 257.9982622000216,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 308.1454007200636,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 14.921726311184232,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 15.290361589927834,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 16.758674872708347,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 17.58068898473938,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "e367209f37ceb97b23586cf254887f3784320aad",
          "message": "fix ruff",
          "timestamp": "2024-03-15T19:40:07Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/e367209f37ceb97b23586cf254887f3784320aad"
        },
        "date": 1710532181693,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 1.0442977430662561,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"input_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 194.44823975893686,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"output_throughput\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 184.63184097411406,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"request_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 6.413840515318209,
            "unit": "prompts/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"token_throughput\",\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 2908.2277048607352,
            "unit": "tokens/s",
            "extra": "{\n  \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_throughput.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput {\\n  \\\"use-all-available-gpus_\\\": \\\"\\\",\\n  \\\"output-len\\\": 128,\\n  \\\"num-prompts\\\": 100,\\n  \\\"dataset\\\": \\\"sharegpt\\\",\\n  \\\"max-model-len\\\": 4096\\n}\",\n    \"backend\": \"vllm\",\n    \"sparsity\": null,\n    \"dataset\": \"sharegpt\",\n    \"input_len\": null,\n    \"output_len\": 128,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"quantization\": null,\n    \"n\": 1,\n    \"use_beam_search\": false,\n    \"num_prompts\": 100,\n    \"num_warmup_prompts\": 1000,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"log_model_io\": false,\n    \"max_model_len\": 4096,\n    \"dtype\": \"auto\",\n    \"enforce_eager\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"tensor_parallel_size_\": null,\n    \"use_all_available_gpus_\": true\n  }\n}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "e367209f37ceb97b23586cf254887f3784320aad",
          "message": "fix ruff",
          "timestamp": "2024-03-15T19:40:07Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/e367209f37ceb97b23586cf254887f3784320aad"
        },
        "date": 1710532183306,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "{\n  \"name\": \"median_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 3422.7453550000746,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4629.274370599978,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_request_latency\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 4770.831266959963,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 214.41646219998347,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 316.33749000002354,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 333.7874883999575,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_ttft_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 344.05791903995123,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"mean_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 15.909269978417177,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"median_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 15.635091895683791,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p90_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 17.504198344661813,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          },
          {
            "name": "{\n  \"name\": \"p99_tpot_ms\",\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  }\n}",
            "value": 18.4472945800351,
            "unit": "ms",
            "extra": "{\n  \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n  \"benchmarking_context\": {\n    \"vllm_version\": \"0.1.0\",\n    \"python_version\": \"3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]\",\n    \"torch_version\": \"2.1.2+cu121\",\n    \"torch_cuda_version\": \"12.1\",\n    \"cuda_devices\": \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\",\n    \"cuda_device_names\": [\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\",\n      \"NVIDIA A10G\"\n    ]\n  },\n  \"gpu_description\": \"NVIDIA A10G x 4\",\n  \"script_name\": \"benchmark_serving.py\",\n  \"script_args\": {\n    \"description\": \"Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax-model-len - 4096\\nsparsity - None\\nbenchmark_serving {\\n  \\\"nr-qps-pair_\\\": \\\"5,inf\\\",\\n  \\\"dataset\\\": \\\"sharegpt\\\"\\n}\",\n    \"backend\": \"vllm\",\n    \"version\": \"N/A\",\n    \"base_url\": null,\n    \"host\": \"localhost\",\n    \"port\": 9000,\n    \"endpoint\": \"/generate\",\n    \"dataset\": \"sharegpt\",\n    \"num_input_tokens\": null,\n    \"num_output_tokens\": null,\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"tokenizer\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"best_of\": 1,\n    \"use_beam_search\": false,\n    \"log_model_io\": false,\n    \"seed\": 0,\n    \"trust_remote_code\": false,\n    \"disable_tqdm\": false,\n    \"save_directory\": \"benchmark-results\",\n    \"num_prompts_\": null,\n    \"request_rate_\": null,\n    \"nr_qps_pair_\": [\n      5,\n      \"inf\"\n    ],\n    \"server_tensor_parallel_size\": 4,\n    \"server_args\": \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"\n  }\n}"
          }
        ]
      }
    ]
  }
}