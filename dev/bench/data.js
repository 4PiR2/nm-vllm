window.BENCHMARK_DATA = {
  "lastUpdate": 1710376492432,
  "repoUrl": "https://github.com/neuralmagic/nm-vllm",
  "entries": {
    "Benchmark": [
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5",
          "message": "fix missing nm-github-action-benchmark",
          "timestamp": "2024-03-13T21:10:21Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5"
        },
        "date": 1710365427048,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "request_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 1.0662640767421154,
            "unit": "prompts/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "input_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 198.53837108938188,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "output_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 188.515488768006,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "request_throughput\nDescription: Benchmark vllm engine throughput - with dataset\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nmax_model_len - 4096\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096\nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 6.433187135893985,
            "unit": "prompts/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_throughput.py', 'script_args': {'description': 'Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096', 'backend': 'vllm', 'sparsity': None, 'dataset': 'sharegpt', 'input_len': None, 'output_len': 128, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'quantization': None, 'n': 1, 'use_beam_search': False, 'num_prompts': 100, 'num_warmup_prompts': 1000, 'seed': 0, 'trust_remote_code': False, 'log_model_io': False, 'max_model_len': 4096, 'dtype': 'auto', 'enforce_eager': False, 'save_directory': 'benchmark-results', 'tensor_parallel_size_': None, 'use_all_available_gpus_': True}}"
          },
          {
            "name": "token_throughput\nDescription: Benchmark vllm engine throughput - with dataset\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nmax_model_len - 4096\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096\nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 2917.0000430284094,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_throughput.py', 'script_args': {'description': 'Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096', 'backend': 'vllm', 'sparsity': None, 'dataset': 'sharegpt', 'input_len': None, 'output_len': 128, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'quantization': None, 'n': 1, 'use_beam_search': False, 'num_prompts': 100, 'num_warmup_prompts': 1000, 'seed': 0, 'trust_remote_code': False, 'log_model_io': False, 'max_model_len': 4096, 'dtype': 'auto', 'enforce_eager': False, 'save_directory': 'benchmark-results', 'tensor_parallel_size_': None, 'use_all_available_gpus_': True}}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5",
          "message": "fix missing nm-github-action-benchmark",
          "timestamp": "2024-03-13T21:10:21Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5"
        },
        "date": 1710365429304,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "median_request_latency\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 3326.5473080000447,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p90_request_latency\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 4529.225166599917,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p99_request_latency\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 4672.029453959958,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "mean_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 155.05685940001968,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "median_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 46.93875400016623,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p90_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 317.8008043999853,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p99_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 317.9234974400151,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "mean_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 14.811843755941906,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "median_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 15.070514525973522,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p90_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 16.748205914778648,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p99_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 17.636156700336507,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5",
          "message": "fix missing nm-github-action-benchmark",
          "timestamp": "2024-03-13T21:10:21Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5"
        },
        "date": 1710366968636,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "request_throughput\nDescription: Benchmark vllm engine throughput - with dataset\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nmax_model_len - 4096\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096\nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 6.447444959918752,
            "unit": "prompts/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_throughput.py', 'script_args': {'description': 'Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096', 'backend': 'vllm', 'sparsity': None, 'dataset': 'sharegpt', 'input_len': None, 'output_len': 128, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'quantization': None, 'n': 1, 'use_beam_search': False, 'num_prompts': 100, 'num_warmup_prompts': 1000, 'seed': 0, 'trust_remote_code': False, 'log_model_io': False, 'max_model_len': 4096, 'dtype': 'auto', 'enforce_eager': False, 'save_directory': 'benchmark-results', 'tensor_parallel_size_': None, 'use_all_available_gpus_': True}}"
          },
          {
            "name": "token_throughput\nDescription: Benchmark vllm engine throughput - with dataset\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nmax_model_len - 4096\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096\nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 2923.46496817596,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_throughput.py', 'script_args': {'description': 'Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096', 'backend': 'vllm', 'sparsity': None, 'dataset': 'sharegpt', 'input_len': None, 'output_len': 128, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'quantization': None, 'n': 1, 'use_beam_search': False, 'num_prompts': 100, 'num_warmup_prompts': 1000, 'seed': 0, 'trust_remote_code': False, 'log_model_io': False, 'max_model_len': 4096, 'dtype': 'auto', 'enforce_eager': False, 'save_directory': 'benchmark-results', 'tensor_parallel_size_': None, 'use_all_available_gpus_': True}}"
          },
          {
            "name": "request_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 1.054559002430426,
            "unit": "prompts/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "input_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 196.35888625254532,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "output_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 186.44603162969932,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5",
          "message": "fix missing nm-github-action-benchmark",
          "timestamp": "2024-03-13T21:10:21Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5"
        },
        "date": 1710366970320,
        "tool": "customSmallerIsBetter",
        "benches": [
          {
            "name": "median_request_latency\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 3374.679899999933,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p90_request_latency\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 4581.700585599992,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p99_request_latency\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 4724.150344959962,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "mean_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 159.0821784000127,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "median_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 51.600776999976006,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p90_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 320.87884140000824,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p99_ttft_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 320.9882828400032,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "mean_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 15.074446168166014,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "median_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 15.224493327921934,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p90_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 17.003888442564044,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "p99_tpot_ms\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 17.94074257716726,
            "unit": "ms",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          }
        ]
      },
      {
        "commit": {
          "author": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "committer": {
            "name": "Varun Sundar Rabindranath",
            "email": "varun@neuralmagic.com"
          },
          "id": "82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5",
          "message": "fix missing nm-github-action-benchmark",
          "timestamp": "2024-03-13T21:10:21Z",
          "url": "https://github.com/neuralmagic/nm-vllm/commit/82db7dc44ec5cbf4b902d53807c1da9c7c7bc6a5"
        },
        "date": 1710376491869,
        "tool": "customBiggerIsBetter",
        "benches": [
          {
            "name": "request_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 1.0468046332953327,
            "unit": "prompts/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "input_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 194.91502271959092,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "output_throughput\nDescription: Benchmark vllm serving\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nsparsity - None\nmax_model_len - 4096\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests \nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 185.0750591666148,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_serving.py', 'script_args': {'description': 'Benchmark vllm serving\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nsparsity - None\\nmax_model_len - 4096\\nbenchmark_serving --nr-qps-pair_ 5,inf --dataset sharegpt\\nserver-cmd : python3 -m vllm.entrypoints.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --tokenizer mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 4096 --host localhost --port 9000 --tensor-parallel-size 4 --disable-log-requests ', 'backend': 'vllm', 'version': 'N/A', 'base_url': None, 'host': 'localhost', 'port': 9000, 'endpoint': '/generate', 'dataset': 'sharegpt', 'num_input_tokens': None, 'num_output_tokens': None, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'best_of': 1, 'use_beam_search': False, 'log_model_io': False, 'seed': 0, 'trust_remote_code': False, 'disable_tqdm': False, 'save_directory': 'benchmark-results', 'num_prompts_': None, 'request_rate_': None, 'nr_qps_pair_': [5, 'inf'], 'server_tensor_parallel_size': 4, 'server_args': \"{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'max-model-len': 4096, 'host': 'localhost', 'port': 9000, 'tensor-parallel-size': 4, 'disable-log-requests': ''}\"}}"
          },
          {
            "name": "request_throughput\nDescription: Benchmark vllm engine throughput - with dataset\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nmax_model_len - 4096\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096\nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 6.397421904482109,
            "unit": "prompts/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_throughput.py', 'script_args': {'description': 'Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096', 'backend': 'vllm', 'sparsity': None, 'dataset': 'sharegpt', 'input_len': None, 'output_len': 128, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'quantization': None, 'n': 1, 'use_beam_search': False, 'num_prompts': 100, 'num_warmup_prompts': 1000, 'seed': 0, 'trust_remote_code': False, 'log_model_io': False, 'max_model_len': 4096, 'dtype': 'auto', 'enforce_eager': False, 'save_directory': 'benchmark-results', 'tensor_parallel_size_': None, 'use_all_available_gpus_': True}}"
          },
          {
            "name": "token_throughput\nDescription: Benchmark vllm engine throughput - with dataset\nmodel - mistralai/Mistral-7B-Instruct-v0.2\nmax_model_len - 4096\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096\nGPU : NVIDIA A10G x 4\nContext : {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}",
            "value": 2900.7830141493228,
            "unit": "tokens/s",
            "extra": "{'benchmarking_context': {'vllm_version': '0.1.0', 'python_version': '3.10.12 (main, Mar  7 2024, 18:39:53) [GCC 9.4.0]', 'torch_version': '2.1.2+cu121', 'torch_cuda_version': '12.1', 'cuda_devices': \"[_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80), _CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)]\", 'cuda_device_names': ['NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G', 'NVIDIA A10G']}, 'script_name': 'benchmark_throughput.py', 'script_args': {'description': 'Benchmark vllm engine throughput - with dataset\\nmodel - mistralai/Mistral-7B-Instruct-v0.2\\nmax_model_len - 4096\\nbenchmark_throughput --use-all-available-gpus_ --output-len 128 --num-prompts 100 --dataset sharegpt --max-model-len 4096', 'backend': 'vllm', 'sparsity': None, 'dataset': 'sharegpt', 'input_len': None, 'output_len': 128, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2', 'quantization': None, 'n': 1, 'use_beam_search': False, 'num_prompts': 100, 'num_warmup_prompts': 1000, 'seed': 0, 'trust_remote_code': False, 'log_model_io': False, 'max_model_len': 4096, 'dtype': 'auto', 'enforce_eager': False, 'save_directory': 'benchmark-results', 'tensor_parallel_size_': None, 'use_all_available_gpus_': True}}"
          }
        ]
      }
    ]
  }
}